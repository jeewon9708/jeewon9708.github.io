---
layout: post
title:  "Relation prediction paper 코드 분석 및 실험 재연"
date:   2020-10-15T16:25:52-23:00
author: Jeewon Chae
categories: Paper_review
use_math: true
---



해당 블로그 포스트는 ACL 2019 에 게재된 [Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs](https://www.aclweb.org/anthology/P19-1466/) 논문의 심층 분석과 실험 재연을 정리하기 위해 작성되었습니다. 실험 코드는 https://github.com/deepakn97/relationPrediction를 참고하면 됩니다.

### Input 데이터 입력

아래의 input 데이터 예시는 kinship dataset 예시입니다.

* entity2id.txt

  entity의 string 형태의 이름과 id의 매핑 정보

  ```
  person37	2
  person90	16
  person99	48
  person47	81
  person9		24
  person1		53
  ...
  ```

  위의 파일은 읽고 **entity2id라는 dictionary 형태의 자료형**으로 저장됩니다.

* entity2vec.txt

  entity의 벡터형태

  ```
  -0.064141	-0.041612	-0.043676	-0.014604	-0.004814 ....
  -0.007791	0.045878	0.029391	-0.007300	-0.016744 ....
  0.004756	0.054812	0.030807	-0.037645	0.086652 ....
  -0.017819	0.028565	0.044253	-0.031128	0.075392 ....
  -0.019164	0.041431	-0.116642	-0.036461	0.054907 ....
  ...
  ```

  위의 파일을 읽고 entity 임베딩을 추출하여 저장합니다. **entity_emb라는 데이터에 저장되며 자료형은 numpy array 타입**입니다. 

* relation2id.txt

  relation의 string 형태의 이름과 id의 매핑정보

  ```
  term6	0
  term21	20
  term8	6
  term11	5
  term9	10
  ...
  ```

  위의 파일은 읽고 **relation2id라는 dictionary 형태의 자료형**으로 저장됩니다.

* relation2vec.txt

  relation의 벡터형태

  ```
  0.107004	-0.096198	-0.091450	0.046713	0.011882 ....
  -0.150780	0.031612	0.030272	0.004528	0.019943 ....
  0.118496	0.016978	-0.021675	0.092790	-0.073582 ....
  0.133777	-0.018250	-0.030474	-0.023850	-0.069799 ....
  -0.049039	-0.004088	-0.005087	-0.113593	0.081364 ....
  ...
  ```

  위의 파일을 읽고 relation 임베딩을 추출하여 저장합니다. **relation_emb라는 데이터에 저장되며 자료형은 numpy array 타입**입니다. 

* train.txt

  train set의 triple 정보

  ```
  person84	term21	person85
  person20	term11	person90
  person64	term7	person80
  person21	term10	person51
  person93	term7	person74
  ...
  ```

  위와 같은 triple 데이터를 읽을 때에는 라인별로 파싱한 후 **triples_data라는 list형태의 자료형**에 저장합니다. 또한 triple의 형태를 그래프로 나타내기위해서 **그래프 자료구조**에 추가합니다. 그래프 자료구조는 먼저 rows,cols,data 라는 리스트 자료형을 만들고 rows에는 head entity, cols에는 tail entity 그리고 data에는 relation을 저장합니다. 이 때, 저장하는 값은 entity나 relation의 이름이 아니라 각각의 id의 형태로 저장됩니다. 

* valid.txt

  validation set의 triple 정보 (위의 train.txt와 같은 형태)

* test.txt

  test set의 triple 정보 (위의 train.txt와 같은 형태)

### Train 과정

위의 입력 데이터를 받았으면 이제 학습을 시킵니다. KBAT 모델은 지난번 포스팅에서 설명하였듯이 GAT(graph attention model)가 encoder, ConvKB가 decoder로 동작하는 encoder-decoder 모델이다. 따라서 학습과정에서도 먼저 GAT로 학습하고 후에 ConvKB로 학습한다. 

```
train_gat(args)
train_conv(args)
```

##### GAT

먼저 train_gat 함수에서 encoding 과정을 수행한다 . 모델은 지난번 포스팅에 정리해놓았습니다.

* **optimizer**: Adam

* **scheduler:** optimum.lr_scheduler.StepLR

* **loss function**: Margin Ranking Loss function

##### ConvKB

decoding 과정을 수행하는 ConvKB 모델은 아래와 같이 진행됩니다. 

* **optimizer**: Adam
* **scheduler:** optimum.lr_scheduler.StepLR
* **loss function**: Soft margin loss function



##### N-hop neighbor

KBAT에서 가장 중요한 **n-hop neighbor**를 어떻게 처리하는지를 살펴보겠습니다.

n-hop의 neighbors에 대해 모두 처리해야하므로 get_batch_nhop_neighbors_all 함수에서  entity의 n_hop 리스트를 numpy array인  batch_source_triples에 추가하고 이를 아래와 같이 current_batch_2hop_indices에 저장합니다. 이를 Variable()로 wrapping하여 노드로 표현하고 이노드를 model을 생성할 때 추가하여 함께 layer를 생성합니다

```python
current_batch_2hop_indices = Corpus_.get_batch_nhop_neighbors_all(args,     Corpus_.unique_entities_train, node_neighbors_2hop)
```

```python
entity_embed, relation_embed = model_gat( Corpus_, Corpus_.train_adj_matrix, train_indices, current_batch_2hop_indices)
```

### Optimizer와 scheduler

딥러닝에서 optimization은 학습속도를 빠르고 안정적이게 하는 것입니다. 

![a;lkfj](https://user-images.githubusercontent.com/22410209/96254917-0cba4100-0ff1-11eb-981d-91fcadd8c260.JPG)

위의 식은 가장 기초적인 Gradient Descent 입니다. 위의 방법에서 learning rate와 gradient를 둘다 수정하여 만든 optimizer가 Adam입니다. 처음에 빠르게 학습하고 나중에 세밀하게 학습하는 Adagrad와 관성 개념을 도입하여 방향을 잘 설정한 momentum의 결합으로 좋은 성능을 보이는 optimizer입니다. 

학습이 진행되면서 learing rate를 그 상황에 맞게 가변적으로 적당하게 변경하는  것이 바로 스케줄러가 하는 일입니다. learning rate가 너무 크면 발산하게 되고 너무 작으면 너무 오래걸리기 때문입니다. 따라서 실질적으로 learning rate를 크게 설정하고 점차 줄여나가는 방안으로서 학습합니다.

이 때, StepLR은 정해진 step_size마다 learning rate에 gamma를 곱하여 learning rate를 감소하는 방향으로 진행합니다. 

### Loss function

**GAT**에서 사용하는 loss function은 **Margin Ranking Loss function**이다. Loss function의 수식은 아래와 같다.

$loss(x,y) = max(0,-y*(x1-x2)+margin)$

Ranking loss의 목표는 다른 loss function들과 다르게 inputs 사이의 상대적인 거리를 예측하는 것입니다. 위의 식에서 1D mini-batch의 Tensor x1과 x2, 1과 -1로 이루어진 mini-batch Tensor y 사이의 loss를 측정하는 기준을 생성합니다. 본 코드에서는 pos_norm과 neg_norm 이 x1,x2의 입력값입니다. 

```python
	source_embeds = entity_embed[pos_triples[:, 0]]
    relation_embeds = relation_embed[pos_triples[:, 1]]
    tail_embeds = entity_embed[pos_triples[:, 2]]

    x = source_embeds + relation_embeds - tail_embeds
    pos_norm = torch.norm(x, p=1, dim=1)

    source_embeds = entity_embed[neg_triples[:, 0]]
    relation_embeds = relation_embed[neg_triples[:, 1]]
    tail_embeds = entity_embed[neg_triples[:, 2]]

    x = source_embeds + relation_embeds - tail_embeds
    neg_norm = torch.norm(x, p=1, dim=1)

    y = -torch.ones(int(args.valid_invalid_ratio_gat) * len_pos_triples).cuda()

    loss = gat_loss_func(pos_norm, neg_norm, y)
```



**ConvKB**에서 사용하는 loss function은 **Soft Margin Loss function**이다. Loss function의 수식은 아래와 같습니다.

$loss(x,y) = \Sigma log(1+exp(-y[i]*x[i])) / x.nelement()$



### Evaluation Protocol 

본 논문에서 제시한 **KBAT** 의 목표는 **어떠한 triple $(e_i,r_k,e_j)$에  대하여 $(e_i,r_k)$ 만 주어졌을 때 $e_j$를 예측하고  $(r_k,e_j)$ 만 주어졌을 때 $e_i$를 예측하는 것입니다.**

따라서 실험을 위해 $(N-1)$ 개의 triple에 대하여 $e_i$를 $e_i'$으로 바꾸어 $(e_i',r_k,e_j)$인 **corrupt triple**을 생성하고 각 triple에 대하여 점수를 매깁니다. 이 때, $e_i'$는 전체 edge에서 $e_i$를 뺀 나머리 edge들입니다. 

***각 triple에 대해 corrupt된 triple들과 함께 점수를 매기고 valid한 triple의 rank를 얻어 평가합니다.*** 

모든 모델들을 ***filtered* 세팅**으로 평가되었고 이는 ranking을 하는 중에는 이미 train, validation, test 셋에 존재하는 corrupt triple은 제거한다는 뜻입니다. 즉, 각 triple에 대하여 valid한 triple의 rank만 저장하고 이를 아래와 같은 지표들로 평가합니다.

위의 실험은 corrupt triple을 **head entity** $e_i$를 $e_i'$로 바꾸는 경우, tail entity  $e_j$를 $e_j'$로 바꾸는 경우, 그리고 **평균**으로 나타납니다. 

**Mean reciprocal Rank(MRR)**, **mean rank(MR)**, **Hits@N** 이라는 top N rank에 있는 올바른 entity의 개수를 지표로 평가합니다. 

**참고** : Evaluate를 할 때에는 ConvKB Only로 평가합니다.



**Mean reciprocal rank** 는 맞을 확률의 평균으로  valid한 triple의 rank가 k일 때 1/k를 지표로 하고 이의 평균입니다. 

Ex) 

triple $(e_{i1},r_{k1},e_{j1})$ , $(e_{i2},r_{k2},e_{j2})$, $(e_{i3},r_{k3},e_{j3})$의 각 rank가 2등 5등 3등이라면 (1/2 + 1/5 + 1/3) / 3 으로 MRR = 0.34이다. 즉, Valid한 triple이 높은 등수에 있을 수록 결과가 좋은 것이므로 MRR의 값은 작을수록 학습 결과가 좋은 것입니다.

아래는 전체 결과 MRR을 구하는 식입니다.

```
cumulative_mean_recip_rank = (sum(average_mean_recip_rank_head) / len(average_mean_recip_rank_head) + sum(
            average_mean_recip_rank_tail) / len(average_mean_recip_rank_tail)) / 2
```



**Mean rank**는 valid한 triple의 rank가 k일 때 이의 평균입니다. 위의 MRR의 예시에서 보면 rank가 2등 5등 3등이므로 MR=(2+5+3)/3=0.33입니다. Valid한 tripe이 높은 등수에 있을수록 결과가 좋은 것이므로 MR의 값은 작은수록 학습결과가 좋은 것입니다. 

아래는 전체 결과 MR을 구하는 식입니다.

```
 cumulative_mean_rank = (sum(average_mean_rank_head) / len(average_mean_rank_head)
                                + sum(average_mean_rank_tail) / len(average_mean_rank_tail)) / 2
```



**Hits@N**는 각 triple마다 corrupt triple을 만들어놓았으므로 올바른 triple이 top N에 들어간는 비율입니다. 예를 들어 Hits@1은 각 경우의 올바른 triple이 rank 1에 있는 비율이고 Hits@3은 각 경우의 올바른 triple이 rank 3 안에 있는 비율이다. 따라서 Hits@N이 클수록 학습 결과가 좋은 것이다. 

```
cumulative_hits_100 = (sum(average_hits_at_100_head) / len(average_hits_at_100_head) + sum(average_hits_at_100_tail) / len(average_hits_at_100_tail)) / 2

cumulative_hits_ten = (sum(average_hits_at_ten_head) / len(average_hits_at_ten_head) + sum(average_hits_at_ten_tail) / len(average_hits_at_ten_tail)) / 2
        
cumulative_hits_three = (sum(average_hits_at_three_head) / len(average_hits_at_three_head) + sum(average_hits_at_three_tail) / len(average_hits_at_three_tail)) / 2

cumulative_hits_one = (sum(average_hits_at_one_head) / len(average_hits_at_one_head) + sum(average_hits_at_one_tail) / len(average_hits_at_one_tail)) / 2
```



### 실험 재연

본 논문의 실험을 재연해보았습니다. 본 논문에서는 WN18RR, FB15k-237,NELL-997,Kinship,UMLS와 같은 다양한 데이터셋으로 실험을 하였으나 WN18RR과 FB15k-237과 같이 크기가 큰 데이터셋의 경우에 실험 관경의 메모리 사이즈의 제한으로 모두 메모리에 올려 학습시킬 수 없었습니다. 아래는 데이터셋의 정보 테이블입니다.

![table_1](https://user-images.githubusercontent.com/22410209/93541878-9515d980-f992-11ea-9c62-1bad9d505571.JPG)



따라서 이 중 가장 데이터가 작은 **Kinship**으로 실험을 진행하였습니다. Parameter 값에 따라 논문의 실험 결과와 약간 다를 수 있지만 대부분 비슷한 결과값을 얻을 수 있었습니다. 

논문의 실험 결과는 아래와 같습니다.

![table_3](https://user-images.githubusercontent.com/22410209/93541905-a52db900-f992-11ea-9b96-bfdaa9a6990d.JPG)

이 때, 제가 깃허브 코드로 실험을 재연하여  구한 결과는 아래와 같습니다.

![kinship_table](https://user-images.githubusercontent.com/22410209/96253950-70436f00-0fef-11eb-91cb-79d4901cc0fc.JPG)

논문의 실험 결과값과 거의 유사한 것을 확인할 수 있습니다. 약간의 다른 점은 parameter 값의 차이라고 생각합니다.

실제로 실험을 한 결과 스크린샷은 아래와 같습니다.

![kinship1](https://user-images.githubusercontent.com/22410209/96254010-8b15e380-0fef-11eb-9cbd-d4d26785b89d.png)



parameter로 입력할 수 있는 값은 아래와 같습니다.

```
--data: Specify the folder name of the dataset.

--epochs_gat: Number of epochs for gat training.

--epochs_conv: Number of epochs for convolution training.

--lr: Initial learning rate.

--weight_decay_gat: L2 reglarization for gat.

--weight_decay_conv: L2 reglarization for conv.

--get_2hop: Get a pickle object of 2 hop neighbors.

--use_2hop: Use 2 hop neighbors for training.

--partial_2hop: Use only 1 2-hop neighbor per node for training.

--output_folder: Path of output folder for saving models.

--batch_size_gat: Batch size for gat model.

--valid_invalid_ratio_gat: Ratio of valid to invalid triples for GAT training.

--drop_gat: Dropout probability for attention layer.

--alpha: LeakyRelu alphas for attention layer.

--nhead_GAT: Number of heads for multihead attention.

--margin: Margin used in hinge loss.

--batch_size_conv: Batch size for convolution model.

--alpha_conv: LeakyRelu alphas for conv layer.

--valid_invalid_ratio_conv: Ratio of valid to invalid triples for conv training.

--out_channels: Number of output channels in conv layer.

--drop_conv: Dropout probability for conv layer.
```

